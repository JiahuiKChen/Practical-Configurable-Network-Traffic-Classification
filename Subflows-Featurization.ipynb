{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Packet Windows/Subflows Basic Feature Extraction (For Comparison/Baseline Experiments)\n",
    "\n",
    "These features are extracted for each set of N-packets for each flow:  \n",
    "\n",
    "    Maximum Inter-Packet Arrival Time\n",
    "    Minimum Inter-Packet Arrival Time\n",
    "    Average Inter-Packet Arrival Time\n",
    "    Standard Deviation of Inter-Packet Arrival Time\n",
    "    Maximum Packet Size\n",
    "    Minimum Packet Size\n",
    "    Average Packet Size\n",
    "    Standard Deviation of Packet Size\n",
    "\n",
    "    \n",
    "Extracted for N = 25, 100, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netaddr\n",
    "import csv\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "#\n",
    "# Extracts a feature vector for each N Packets for each flow in the input file.\n",
    "#\n",
    "# A flow is defined by a unique bi-directional tuple: \n",
    "# {IP Source Address, Source Port, IP Destination Address, Destination Port, Protocol}\n",
    "# \n",
    "# The 15 features above are extracted for each N-Packet subflows for all flows in the given datafile. \n",
    "#\n",
    "def extract_packet_win_features(filename, N, abs_time=False):\n",
    "    # Dictionary holding values for feature calculations with keys of:\n",
    "    # 'ip_source-source_port-ip_dest-dest_port-protocol'\n",
    "    flows = {}\n",
    "    # Returned feature values extracted from the given file (all vectors in the file)\n",
    "    feature_vecs = []\n",
    "    # Feature vectors for each flow in the file (flow -> feature vector list)\n",
    "    # THE LIST OF FEATURE VECTORS IS IN CHRONOLOGICAL ORDERING ACCORDING TO WHEN THE SUBFLOWS ARRIVED\n",
    "    feature_vecs_flows = {} \n",
    "    # Reading in the CSV file of packet captures\n",
    "    with open(filename, 'r') as data_csv:\n",
    "        reader = csv.reader(data_csv, delimiter=',')\n",
    "        # Loop through all lines (all packets), get statistics of every window of N packets\n",
    "        for index, line in enumerate(reader):\n",
    "            # Line/CSV row format (represents a packet): \n",
    "            # [0:Time (Packet Arrival) 1:Source IP 2:Dest IP 3:Protocol \n",
    "            #  4: Length 5:Info 6:Source Port 7: Dest Port]\n",
    "            if 'Time' in line[0]:\n",
    "                continue\n",
    "            # Parse out fields needed to construct key (if not the first CSV line)\n",
    "            ip_src = line[1]\n",
    "            src_port = line[6]\n",
    "            ip_dest = line[2]\n",
    "            dest_port = line[7]\n",
    "            protocol = line[3]\n",
    "            # Convert string of absolute time to decimal time\n",
    "            if abs_time:\n",
    "                a_time = line[0].split(':')\n",
    "                secs = a_time[-1].split('.')\n",
    "                time = float(a_time[0])*3600 + float(a_time[1])*60 + float(secs[0]) + float('0.' + secs[1])\n",
    "            else:\n",
    "                time = float(line[0])\n",
    "            \n",
    "            key = ip_src + '-' + src_port + '-' + ip_dest + '-' + dest_port + '-' + protocol\n",
    "            \n",
    "            # Create new flow and flow statistics if a new flow is encountered\n",
    "            if key not in flows: \n",
    "                flows[key] = {'all_sizes': [], 'all_intervals': [], 'subflow_start': None, \n",
    "                              'last_arrival': None, 'packet_num': 0}\n",
    "            # Update flow statistics based on current packet's information\n",
    "            flow_stats = flows[key]\n",
    "            length = float(line[4])\n",
    "            # Each packet adds to the total number of packets in current subflow\n",
    "            flow_stats['packet_num'] += 1\n",
    "            # Give the subflow/window a starting time if this is the first packet\n",
    "            if flow_stats['subflow_start'] is None:\n",
    "                flow_stats['subflow_start'] = time\n",
    "            # Adding this packet's size to array of all packet sizes\n",
    "            flow_stats['all_sizes'].append(length)\n",
    "            # Adding interval between this packet's arrival and last packet's arrival \n",
    "            if flow_stats['last_arrival'] is None:\n",
    "                flow_stats['all_intervals'].append(time - flow_stats['subflow_start'])\n",
    "            else:\n",
    "                flow_stats['all_intervals'].append(time - flow_stats['last_arrival'])\n",
    "            # If current packet is the last in the current subflow's packet length,\n",
    "            # featurize current stats and reset stats for next subflow \n",
    "            if flow_stats['packet_num'] == N:                \n",
    "                # get the time length of the subflow (in seconds)\n",
    "                subflow_len = time - flow_stats['subflow_start']\n",
    "                all_pckt_sizes = np.array(flow_stats['all_sizes'])\n",
    "                all_pckt_intervals = np.array(flow_stats['all_intervals'])\n",
    "                subflow_features = np.array([np.max(all_pckt_sizes), np.min(all_pckt_sizes), \n",
    "                                             np.std(all_pckt_sizes), np.average(all_pckt_sizes), \n",
    "                                             np.max(all_pckt_intervals), np.min(all_pckt_intervals), \n",
    "                                             np.std(all_pckt_intervals), np.average(all_pckt_intervals)])\n",
    "                feature_vecs.append(subflow_features) \n",
    "                # Add feature vector for N packet subflow to flow's dictionary entry \n",
    "                if key not in feature_vecs_flows:\n",
    "                    feature_vecs_flows[key] = []\n",
    "                feature_vecs_flows[key].append(subflow_features)\n",
    "                # Reset features for next subflow of this flow\n",
    "                flows[key] = {'all_sizes': [], 'all_intervals': [], 'subflow_start': None, \n",
    "                              'last_arrival': None, 'packet_num': 0}\n",
    "            else:\n",
    "                # Update last arrival time only if this packet wasn't the last\n",
    "                flow_stats['last_arrival'] = time\n",
    "    \n",
    "    # Return a list of feature vectors of all vecs in capture file and dict mapping flows -> flow feature vecs\n",
    "    return feature_vecs, feature_vecs_flows \n",
    "# Any window that doesn't fit (last window: there aren't N packets left to create full subflow) is just dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25-Packet Subflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of WIDE-f flows: 22481\n",
      "Total number of WIDE-f subflows: 388000\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "# WIDE F-POINT 25-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Unknown-Data/wide-f/\"\n",
    "N = 25\n",
    "\n",
    "widef_vecs, widef_flow_vecs = extract_packet_win_features(data_dir + 'f-jan3-all.csv', N, abs_time=True)\n",
    "\n",
    "print(f\"Total number of WIDE-f flows: {len(widef_flow_vecs)}\")\n",
    "print(f\"Total number of WIDE-f subflows: {len(widef_vecs)}\")\n",
    "print(widef_vecs[0].shape)\n",
    "\n",
    "# Saving dict of WIDE-f feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-f_{N}_packet_subflow_flow_features.npy', widef_flow_vecs)\n",
    "\n",
    "# Save all WIDE-f feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-f_{N}_packet_subflow_features.npy', widef_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of WIDE-g flows: 24265\n",
      "Total number of WIDE-g subflows: 724554\n"
     ]
    }
   ],
   "source": [
    "# WIDE G-POINT 25-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Unknown-Data/wide-g/\"\n",
    "N = 25\n",
    "\n",
    "wideg_vecs, wideg_flow_vecs = extract_packet_win_features(data_dir + 'g-jan8-all.csv', N, abs_time=True)\n",
    "\n",
    "print(f\"Total number of WIDE-g flows: {len(wideg_flow_vecs)}\")\n",
    "print(f\"Total number of WIDE-g subflows: {len(wideg_vecs)}\")\n",
    "\n",
    "# Saving dict of WIDE-g feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-g_{N}_packet_subflow_flow_features.npy', wideg_flow_vecs)\n",
    "\n",
    "# Save all WIDE-g feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-g_{N}_packet_subflow_features.npy', wideg_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of FDT flows: 40\n",
      "Total number of FDT subflows: 172480\n",
      "(172480, 8)\n"
     ]
    }
   ],
   "source": [
    "# FDT 25 SUBFLOW FEATURZIATION\n",
    "uknown_data_dir = \"../../Known-Data/new-fdt/\"\n",
    "N = 25\n",
    "\n",
    "fdt8gb_vecs, fdt8gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-8gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt16gb_vecs, fdt16gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-16gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt18gb_vecs, fdt18gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-18gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt40gb_vecs, fdt40gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-40gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt47gb_vecs, fdt47gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-47gb.csv', N, abs_time=True)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "fdt_flow_vecs = [fdt8gb_flow_vecs, fdt16gb_flow_vecs, fdt18gb_flow_vecs, \\\n",
    "                    fdt40gb_flow_vecs, fdt47gb_flow_vecs]\n",
    "\n",
    "# Getting flow count for FDT\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in fdt_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "print(f\"Total number of FDT flows: {flow_tot}\")\n",
    "print(f\"Total number of FDT subflows: {subflow_tot}\")\n",
    "\n",
    "# Saving a list of dicts of FDT feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_fdt_{N}_packet_subflow_flow_features.npy', fdt_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all FDT data \n",
    "all_fdt = fdt8gb_vecs + fdt16gb_vecs + fdt18gb_vecs + fdt40gb_vecs + fdt47gb_vecs\n",
    "all_fdt = np.array(all_fdt)\n",
    "print(all_fdt.shape)\n",
    "\n",
    "# Save all FDT feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_fdt_{N}_packet_subflow_features.npy', all_fdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of RClone flows: 27\n",
      "Total number of RClone subflows: 537801\n",
      "(537801, 8)\n"
     ]
    }
   ],
   "source": [
    "# RCLONE 25-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Known-Data/new-rclone/\"\n",
    "N = 25\n",
    "\n",
    "rclone1_vecs, rclone1_flow_vecs = extract_packet_win_features(data_dir + 'rclone1.csv', N)\n",
    "\n",
    "rclone2_vecs, rclone2_flow_vecs = extract_packet_win_features(data_dir + 'rclone2.csv', N)\n",
    "\n",
    "rclone3_vecs, rclone3_flow_vecs = extract_packet_win_features(data_dir + 'rclone3.csv', N)\n",
    "\n",
    "rclone4_vecs, rclone4_flow_vecs = extract_packet_win_features(data_dir + 'rclone4.csv', N)\n",
    "\n",
    "rclone5_vecs, rclone5_flow_vecs = extract_packet_win_features(data_dir + 'rclone5.csv', N)\n",
    "\n",
    "rclone6_vecs, rclone6_flow_vecs = extract_packet_win_features(data_dir + 'rclone6.csv', N)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "rclone_flow_vecs = [rclone1_flow_vecs, rclone2_flow_vecs, rclone3_flow_vecs, \\\n",
    "                    rclone4_flow_vecs, rclone5_flow_vecs, rclone6_flow_vecs]\n",
    "\n",
    "# Getting flow count for RClone\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in rclone_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "print(f\"Total number of RClone flows: {flow_tot}\")\n",
    "print(f\"Total number of RClone subflows: {subflow_tot}\")\n",
    "\n",
    "# Saving a list of dicts of Rclone feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_rclone_{N}_packet_subflow_flow_features.npy', rclone_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all rclone data \n",
    "all_rclone = rclone1_vecs + rclone2_vecs + rclone3_vecs + rclone4_vecs + rclone5_vecs + rclone6_vecs\n",
    "all_rclone = np.array(all_rclone)\n",
    "print(all_rclone.shape)\n",
    "\n",
    "# Save all new RClone feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_rclone_{N}_packet_subflow_features.npy', all_rclone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Globus flows: 115\n",
      "Total number of Globus subflows: 1572839\n",
      "(1572839, 8)\n"
     ]
    }
   ],
   "source": [
    "# GLOBUS 25-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Known-Data/new-globus/\"\n",
    "N = 25\n",
    "\n",
    "globus_10_min_vecs, globus_10_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min.csv', N)\n",
    "\n",
    "globus_10_min1_vecs, globus_10_min1_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min1.csv', N)\n",
    "\n",
    "globus_10_min2_vecs, globus_10_min2_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min2.csv', N)\n",
    "\n",
    "globus_20_min, globus_20_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-20-min-all.csv', N, abs_time=True)\n",
    "\n",
    "globus_5_min, globus_5_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-5-min.csv', N)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "globus_flow_vecs = [globus_10_min_flow_vecs, globus_10_min1_flow_vecs, globus_10_min2_flow_vecs, \\\n",
    "                    globus_20_min_flow_vecs, globus_5_min_flow_vecs]\n",
    "\n",
    "# Getting flow count for Globus\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in globus_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "print(f\"Total number of Globus flows: {flow_tot}\")\n",
    "print(f\"Total number of Globus subflows: {subflow_tot}\")\n",
    "\n",
    "# Saving a list of dicts of Globus feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_globus_{N}_packet_subflow_flow_features.npy', globus_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all globus data \n",
    "all_globus = globus_10_min_vecs + globus_10_min1_vecs + globus_10_min2_vecs + globus_20_min + globus_5_min\n",
    "all_globus = np.array(all_globus)\n",
    "print(all_globus.shape)\n",
    "\n",
    "# Save all new globus feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_globus_{N}_packet_subflow_features.npy', all_globus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unknown Mirror Flows: 1551\n",
      "Total Unknown Mirror Subflows: 657368\n"
     ]
    }
   ],
   "source": [
    "uknown_data_dir = \"../../Unknown-Data/mirror-unknown/\"\n",
    "N = 25\n",
    "\n",
    "unknown1_vecs, unknown1_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown1.csv', N)\n",
    "\n",
    "unknown2_vecs, unknown2_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown2.csv', N)\n",
    "\n",
    "unknown3_vecs, unknown3_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown3-all.csv', N, abs_time=True)\n",
    "\n",
    "unknown4_vecs, unknown4_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown4-all.csv', N, abs_time=True)\n",
    "\n",
    "unknown5_vecs, unknown5_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown5.csv', N)\n",
    "\n",
    "unknown6_vecs, unknown6_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown6.csv', N)\n",
    "\n",
    "unknown7_vecs, unknown7_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown7.csv', N)\n",
    "\n",
    "unknown_flow_vecs = [unknown1_flow_vecs, unknown2_flow_vecs, unknown3_flow_vecs, unknown4_flow_vecs,\n",
    "                    unknown5_flow_vecs, unknown6_flow_vecs, unknown7_flow_vecs]\n",
    "\n",
    "# Number of flows in unknown data (matters due to experimental setup for ensembling unknowns)\n",
    "flow_tot = 0\n",
    "for flow_dict in unknown_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "print(f\"Total Unknown Mirror Flows: {flow_tot}\")\n",
    "\n",
    "all_unknown = unknown1_vecs + unknown2_vecs + unknown3_vecs + unknown4_vecs \\\n",
    "                + unknown5_vecs + unknown6_vecs + unknown7_vecs\n",
    "all_unknown = np.array(all_unknown)\n",
    "print(f\"Total Unknown Mirror Subflows: {len(all_unknown)}\")\n",
    "\n",
    "# Saving unknown feature vectors, all in one array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/mirror_unknown_{N}_packet_subflow_features.npy', all_unknown)\n",
    "\n",
    "# Saving a list of dicts of unknown feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/mirror_unknown_{N}_packet_subflow_flow_features.npy', unknown_flow_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 Packet Subflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of WIDE-f flows: 804\n",
      "Total number of WIDE-f subflows: 7611\n"
     ]
    }
   ],
   "source": [
    "# WIDE F-POINT 1000-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Unknown-Data/wide-f/\"\n",
    "N = 1000\n",
    "\n",
    "widef_vecs, widef_flow_vecs = extract_packet_win_features(data_dir + 'f-jan3-all.csv', N, abs_time=True)\n",
    "\n",
    "print(f\"Total number of WIDE-f flows: {len(widef_flow_vecs)}\")\n",
    "print(f\"Total number of WIDE-f subflows: {len(widef_vecs)}\")\n",
    "\n",
    "# Saving dict of WIDE-f feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-f_{N}_packet_subflow_flow_features.npy', widef_flow_vecs)\n",
    "\n",
    "# Save all WIDE-f feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-f_{N}_packet_subflow_features.npy', widef_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of WIDE-g flows: 1409\n",
      "Total number of WIDE-g subflows: 15454\n"
     ]
    }
   ],
   "source": [
    "# WIDE G-POINT 1000-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Unknown-Data/wide-g/\"\n",
    "N = 1000\n",
    "\n",
    "wideg_vecs, wideg_flow_vecs = extract_packet_win_features(data_dir + 'g-jan8-all.csv', N, abs_time=True)\n",
    "\n",
    "print(f\"Total number of WIDE-g flows: {len(wideg_flow_vecs)}\")\n",
    "print(f\"Total number of WIDE-g subflows: {len(wideg_vecs)}\")\n",
    "\n",
    "# Saving dict of WIDE-g feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-g_{N}_packet_subflow_flow_features.npy', wideg_flow_vecs)\n",
    "\n",
    "# Save all WIDE-g feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/wide-g_{N}_packet_subflow_features.npy', wideg_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unknown Mirror Flows: 1188\n",
      "Total Unknown Mirror Subflows: 15725\n"
     ]
    }
   ],
   "source": [
    "# MIRROR UNKNOWN 1000-PACKET SUBFLOW FEATURIZATION\n",
    "uknown_data_dir = \"../../Unknown-Data/mirror-unknown/\"\n",
    "N = 1000\n",
    "\n",
    "unknown1_vecs, unknown1_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown1.csv', N)\n",
    "\n",
    "unknown2_vecs, unknown2_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown2.csv', N)\n",
    "\n",
    "unknown3_vecs, unknown3_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown3-all.csv', N, abs_time=True)\n",
    "\n",
    "unknown4_vecs, unknown4_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown4-all.csv', N, abs_time=True)\n",
    "\n",
    "unknown5_vecs, unknown5_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown5.csv', N)\n",
    "\n",
    "unknown6_vecs, unknown6_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown6.csv', N)\n",
    "\n",
    "unknown7_vecs, unknown7_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown7.csv', N)\n",
    "\n",
    "unknown_flow_vecs = [unknown1_flow_vecs, unknown2_flow_vecs, unknown3_flow_vecs, unknown4_flow_vecs,\n",
    "                    unknown5_flow_vecs, unknown6_flow_vecs, unknown7_flow_vecs]\n",
    "\n",
    "# Number of flows in unknown data (matters due to experimental setup for ensembling unknowns)\n",
    "flow_tot = 0\n",
    "for flow_dict in unknown_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "print(f\"Total Unknown Mirror Flows: {flow_tot}\")\n",
    "\n",
    "all_unknown = unknown1_vecs + unknown2_vecs + unknown3_vecs + unknown4_vecs \\\n",
    "                + unknown5_vecs + unknown6_vecs + unknown7_vecs\n",
    "all_unknown = np.array(all_unknown)\n",
    "print(f\"Total Unknown Mirror Subflows: {len(all_unknown)}\")\n",
    "\n",
    "# Saving unknown feature vectors, all in one array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/mirror_unknown_{N}_packet_subflow_features.npy', all_unknown)\n",
    "\n",
    "# Saving a list of dicts of unknown feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/mirror_unknown_{N}_packet_subflow_flow_features.npy', unknown_flow_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of FDT flows: 40\n",
      "Total number of FDT subflows: 4292\n",
      "(4292, 8)\n"
     ]
    }
   ],
   "source": [
    "# FDT 1000 SUBFLOW FEATURZIATION\n",
    "uknown_data_dir = \"../../Known-Data/new-fdt/\"\n",
    "N = 1000\n",
    "\n",
    "fdt8gb_vecs, fdt8gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-8gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt16gb_vecs, fdt16gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-16gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt18gb_vecs, fdt18gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-18gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt40gb_vecs, fdt40gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-40gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt47gb_vecs, fdt47gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-47gb.csv', N, abs_time=True)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "fdt_flow_vecs = [fdt8gb_flow_vecs, fdt16gb_flow_vecs, fdt18gb_flow_vecs, \\\n",
    "                    fdt40gb_flow_vecs, fdt47gb_flow_vecs]\n",
    "\n",
    "# Getting flow count for FDT\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in fdt_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "print(f\"Total number of FDT flows: {flow_tot}\")\n",
    "print(f\"Total number of FDT subflows: {subflow_tot}\")\n",
    "\n",
    "# Saving a list of dicts of FDT feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_fdt_{N}_packet_subflow_flow_features.npy', fdt_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all FDT data \n",
    "all_fdt = fdt8gb_vecs + fdt16gb_vecs + fdt18gb_vecs + fdt40gb_vecs + fdt47gb_vecs\n",
    "all_fdt = np.array(all_fdt)\n",
    "print(all_fdt.shape)\n",
    "\n",
    "# Save all FDT feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_fdt_{N}_packet_subflow_features.npy', all_fdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of RClone flows: 25\n",
      "Total number of RClone subflows: 13434\n",
      "(13434, 8)\n"
     ]
    }
   ],
   "source": [
    "# RCLONE 1000-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Known-Data/new-rclone/\"\n",
    "N = 1000\n",
    "\n",
    "rclone1_vecs, rclone1_flow_vecs = extract_packet_win_features(data_dir + 'rclone1.csv', N)\n",
    "\n",
    "rclone2_vecs, rclone2_flow_vecs = extract_packet_win_features(data_dir + 'rclone2.csv', N)\n",
    "\n",
    "rclone3_vecs, rclone3_flow_vecs = extract_packet_win_features(data_dir + 'rclone3.csv', N)\n",
    "\n",
    "rclone4_vecs, rclone4_flow_vecs = extract_packet_win_features(data_dir + 'rclone4.csv', N)\n",
    "\n",
    "rclone5_vecs, rclone5_flow_vecs = extract_packet_win_features(data_dir + 'rclone5.csv', N)\n",
    "\n",
    "rclone6_vecs, rclone6_flow_vecs = extract_packet_win_features(data_dir + 'rclone6.csv', N)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "rclone_flow_vecs = [rclone1_flow_vecs, rclone2_flow_vecs, rclone3_flow_vecs, \\\n",
    "                    rclone4_flow_vecs, rclone5_flow_vecs, rclone6_flow_vecs]\n",
    "\n",
    "# Getting flow count for RClone\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in rclone_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "print(f\"Total number of RClone flows: {flow_tot}\")\n",
    "print(f\"Total number of RClone subflows: {subflow_tot}\")\n",
    "\n",
    "# Saving a list of dicts of Rclone feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_rclone_{N}_packet_subflow_flow_features.npy', rclone_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all rclone data \n",
    "all_rclone = rclone1_vecs + rclone2_vecs + rclone3_vecs + rclone4_vecs + rclone5_vecs + rclone6_vecs\n",
    "all_rclone = np.array(all_rclone)\n",
    "print(all_rclone.shape)\n",
    "\n",
    "# Save all new RClone feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_rclone_{N}_packet_subflow_features.npy', all_rclone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Globus flows: 107\n",
      "Total number of Globus subflows: 39270\n",
      "(39270, 8)\n"
     ]
    }
   ],
   "source": [
    "# GLOBUS 1000-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Known-Data/new-globus/\"\n",
    "N = 1000\n",
    "\n",
    "globus_10_min_vecs, globus_10_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min.csv', N)\n",
    "\n",
    "globus_10_min1_vecs, globus_10_min1_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min1.csv', N)\n",
    "\n",
    "globus_10_min2_vecs, globus_10_min2_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min2.csv', N)\n",
    "\n",
    "globus_20_min, globus_20_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-20-min-all.csv', N, abs_time=True)\n",
    "\n",
    "globus_5_min, globus_5_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-5-min.csv', N)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "globus_flow_vecs = [globus_10_min_flow_vecs, globus_10_min1_flow_vecs, globus_10_min2_flow_vecs, \\\n",
    "                    globus_20_min_flow_vecs, globus_5_min_flow_vecs]\n",
    "\n",
    "# Getting flow count for Globus\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in globus_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "print(f\"Total number of Globus flows: {flow_tot}\")\n",
    "print(f\"Total number of Globus subflows: {subflow_tot}\")\n",
    "\n",
    "# Saving a list of dicts of Globus feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_globus_{N}_packet_subflow_flow_features.npy', globus_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all globus data \n",
    "all_globus = globus_10_min_vecs + globus_10_min1_vecs + globus_10_min2_vecs + globus_20_min + globus_5_min\n",
    "all_globus = np.array(all_globus)\n",
    "print(all_globus.shape)\n",
    "\n",
    "# Save all new globus feature vecs in one np array\n",
    "np.save(f'../../Feature-Vectors/Basic-Features/new_globus_{N}_packet_subflow_features.npy', all_globus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100-Packet Subflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of WIDE-f flows: 4937\n",
      "Total number of WIDE-f subflows: 89528\n"
     ]
    }
   ],
   "source": [
    "# WIDE F-POINT 100-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Unknown-Data/wide-f/\"\n",
    "N = 100\n",
    "\n",
    "widef_vecs, widef_flow_vecs = extract_packet_win_features(data_dir + 'f-jan3-all.csv', N, abs_time=True)\n",
    "\n",
    "print(f\"Total number of WIDE-f flows: {len(widef_flow_vecs)}\")\n",
    "print(f\"Total number of WIDE-f subflows: {len(widef_vecs)}\")\n",
    "\n",
    "# LOTS OF FLOWS (almost 5k) but fewler SUBFLOWS (~90k)\n",
    "\n",
    "# Saving dict of WIDE-f feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save('../../Feature-Vectors/Basic-Features/wide-f_100_packet_subflow_flow_features.npy', widef_flow_vecs)\n",
    "\n",
    "# Save all WIDE-f feature vecs in one np array\n",
    "np.save('../../Feature-Vectors/Basic-Features/wide-f_100_packet_subflow_features.npy', widef_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of WIDE-g flows: 6295\n",
      "Total number of WIDE-g subflows: 173093\n"
     ]
    }
   ],
   "source": [
    "# WIDE G-POINT 100-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Unknown-Data/wide-g/\"\n",
    "N = 100\n",
    "\n",
    "wideg_vecs, wideg_flow_vecs = extract_packet_win_features(data_dir + 'g-jan8-all.csv', N, abs_time=True)\n",
    "\n",
    "print(f\"Total number of WIDE-g flows: {len(wideg_flow_vecs)}\")\n",
    "print(f\"Total number of WIDE-g subflows: {len(wideg_vecs)}\")\n",
    "\n",
    "# Saving dict of WIDE-g feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save('../../Feature-Vectors/Basic-Features/wide-g_100_packet_subflow_flow_features.npy', wideg_flow_vecs)\n",
    "\n",
    "# Save all WIDE-g feature vecs in one np array\n",
    "np.save('../../Feature-Vectors/Basic-Features/wide-g_100_packet_subflow_features.npy', wideg_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of FDT flows: 40\n",
      "Total number of FDT subflows: 43105\n",
      "(43105, 8)\n"
     ]
    }
   ],
   "source": [
    "# FDT 100 SUBFLOW FEATURZIATION\n",
    "uknown_data_dir = \"../../Known-Data/new-fdt/\"\n",
    "N = 100\n",
    "\n",
    "fdt8gb_vecs, fdt8gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-8gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt16gb_vecs, fdt16gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-16gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt18gb_vecs, fdt18gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-18gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt40gb_vecs, fdt40gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-40gb.csv', N, abs_time=True)\n",
    "\n",
    "fdt47gb_vecs, fdt47gb_flow_vecs = extract_packet_win_features(uknown_data_dir + 'fdt-47gb.csv', N, abs_time=True)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "fdt_flow_vecs = [fdt8gb_flow_vecs, fdt16gb_flow_vecs, fdt18gb_flow_vecs, \\\n",
    "                    fdt40gb_flow_vecs, fdt47gb_flow_vecs]\n",
    "\n",
    "# Getting flow count for FDT\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in fdt_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "#         print(len(flow_dict[f]))\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "#     print(len(flow_dict))\n",
    "print(f\"Total number of FDT flows: {flow_tot}\")\n",
    "print(f\"Total number of FDT subflows: {subflow_tot}\")\n",
    "\n",
    "# NOTES\n",
    "\n",
    "# Saving a list of dicts of FDT feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save('../../Feature-Vectors/Basic-Features/new_fdt_100_packet_subflow_flow_features.npy', fdt_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all FDT data \n",
    "all_fdt = fdt8gb_vecs + fdt16gb_vecs + fdt18gb_vecs + fdt40gb_vecs + fdt47gb_vecs\n",
    "all_fdt = np.array(all_fdt)\n",
    "print(all_fdt.shape)\n",
    "\n",
    "# Save all FDT feature vecs in one np array\n",
    "np.save('../../Feature-Vectors/Basic-Features/new_fdt_100_packet_subflow_features.npy', all_fdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of RClone flows: 27\n",
      "Total number of RClone subflows: 134441\n",
      "(134441, 8)\n"
     ]
    }
   ],
   "source": [
    "# RCLONE 100-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Known-Data/new-rclone/\"\n",
    "N = 100\n",
    "\n",
    "rclone1_vecs, rclone1_flow_vecs = extract_packet_win_features(data_dir + 'rclone1.csv', N)\n",
    "\n",
    "rclone2_vecs, rclone2_flow_vecs = extract_packet_win_features(data_dir + 'rclone2.csv', N)\n",
    "\n",
    "rclone3_vecs, rclone3_flow_vecs = extract_packet_win_features(data_dir + 'rclone3.csv', N)\n",
    "\n",
    "rclone4_vecs, rclone4_flow_vecs = extract_packet_win_features(data_dir + 'rclone4.csv', N)\n",
    "\n",
    "rclone5_vecs, rclone5_flow_vecs = extract_packet_win_features(data_dir + 'rclone5.csv', N)\n",
    "\n",
    "rclone6_vecs, rclone6_flow_vecs = extract_packet_win_features(data_dir + 'rclone6.csv', N)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "rclone_flow_vecs = [rclone1_flow_vecs, rclone2_flow_vecs, rclone3_flow_vecs, \\\n",
    "                    rclone4_flow_vecs, rclone5_flow_vecs, rclone6_flow_vecs]\n",
    "\n",
    "# Getting flow count for RClone\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in rclone_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "#         print(len(flow_dict[f]))\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "#     print(len(flow_dict))\n",
    "print(f\"Total number of RClone flows: {flow_tot}\")\n",
    "print(f\"Total number of RClone subflows: {subflow_tot}\")\n",
    "\n",
    "# V FEW RCLONE FLOWS (27, wherease 107 Globus & 1441 unknown)\n",
    "# so these are real long flows...\n",
    "\n",
    "# Saving a list of dicts of Rclone feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save('../../Feature-Vectors/Basic-Features/new_rclone_100_packet_subflow_flow_features.npy', rclone_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all rclone data \n",
    "all_rclone = rclone1_vecs + rclone2_vecs + rclone3_vecs + rclone4_vecs + rclone5_vecs + rclone6_vecs\n",
    "all_rclone = np.array(all_rclone)\n",
    "print(all_rclone.shape)\n",
    "\n",
    "# Save all new RClone feature vecs in one np array\n",
    "np.save('../../Feature-Vectors/Basic-Features/new_rclone_100_packet_subflow_features.npy', all_rclone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Globus flows: 115\n",
      "Total number of Globus subflows: 393166\n",
      "(393166, 8)\n"
     ]
    }
   ],
   "source": [
    "# GLOBUS 100-SUBFLOW FEATURIZATION\n",
    "data_dir = \"../../Known-Data/new-globus/\"\n",
    "N = 100\n",
    "\n",
    "globus_10_min_vecs, globus_10_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min.csv', N)\n",
    "\n",
    "globus_10_min1_vecs, globus_10_min1_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min1.csv', N)\n",
    "\n",
    "globus_10_min2_vecs, globus_10_min2_flow_vecs = extract_packet_win_features(data_dir + 'globus-10-min2.csv', N)\n",
    "\n",
    "globus_20_min, globus_20_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-20-min-all.csv', N, abs_time=True)\n",
    "\n",
    "globus_5_min, globus_5_min_flow_vecs = extract_packet_win_features(data_dir + 'globus-5-min.csv', N)\n",
    "\n",
    "# Creating & saving list of all subflow feature vector lists, each mapped to their flow\n",
    "globus_flow_vecs = [globus_10_min_flow_vecs, globus_10_min1_flow_vecs, globus_10_min2_flow_vecs, \\\n",
    "                    globus_20_min_flow_vecs, globus_5_min_flow_vecs]\n",
    "\n",
    "# Getting flow count for Globus\n",
    "flow_tot = 0\n",
    "subflow_tot = 0\n",
    "for flow_dict in globus_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "    for f in flow_dict:\n",
    "#         print(len(flow_dict[f]))\n",
    "        subflow_tot += len(flow_dict[f])\n",
    "#     print(len(flow_dict))\n",
    "print(f\"Total number of Globus flows: {flow_tot}\")\n",
    "print(f\"Total number of Globus subflows: {subflow_tot}\")\n",
    "\n",
    "# WAY LESS FLOWS FOR GLOBUS (107 vs. 1441 for unknown)\n",
    "# so these are real long flows...\n",
    "\n",
    "# Saving a list of dicts of Globus feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save('../../Feature-Vectors/Basic-Features/new_globus_100_packet_subflow_flow_features.npy', globus_flow_vecs)\n",
    "\n",
    "# Creating & saving list of all globus data \n",
    "all_globus = globus_10_min_vecs + globus_10_min1_vecs + globus_10_min2_vecs + globus_20_min + globus_5_min\n",
    "all_globus = np.array(all_globus)\n",
    "print(all_globus.shape)\n",
    "\n",
    "# Save all new globus feature vecs in one np array\n",
    "np.save('../../Feature-Vectors/Basic-Features/new_globus_100_packet_subflow_features.npy', all_globus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unknown Mirror Flows: 1490\n",
      "Total Unknown Mirror Subflows: 163772\n"
     ]
    }
   ],
   "source": [
    "# MIRROR 100-SUBFLOW FEATURIZATION\n",
    "uknown_data_dir = \"../../Unknown-Data/mirror-unknown/\"\n",
    "N = 100\n",
    "\n",
    "unknown1_vecs, unknown1_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown1.csv', N)\n",
    "\n",
    "unknown2_vecs, unknown2_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown2.csv', N)\n",
    "\n",
    "unknown3_vecs, unknown3_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown3-all.csv', N, abs_time=True)\n",
    "\n",
    "unknown4_vecs, unknown4_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown4-all.csv', N, abs_time=True)\n",
    "\n",
    "unknown5_vecs, unknown5_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown5.csv', N)\n",
    "\n",
    "unknown6_vecs, unknown6_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown6.csv', N)\n",
    "\n",
    "unknown7_vecs, unknown7_flow_vecs = extract_packet_win_features(uknown_data_dir + 'unknown7.csv', N)\n",
    "\n",
    "unknown_flow_vecs = [unknown1_flow_vecs, unknown2_flow_vecs, unknown3_flow_vecs, unknown4_flow_vecs,\n",
    "                    unknown5_flow_vecs, unknown6_flow_vecs, unknown7_flow_vecs]\n",
    "\n",
    "# Number of flows in unknown data (matters due to experimental setup for ensembling unknowns)\n",
    "flow_tot = 0\n",
    "for flow_dict in unknown_flow_vecs:\n",
    "    flow_tot += len(flow_dict)\n",
    "print(f\"Total Unknown Mirror Flows: {flow_tot}\")\n",
    "\n",
    "all_unknown = unknown1_vecs + unknown2_vecs + unknown3_vecs + unknown4_vecs \\\n",
    "                + unknown5_vecs + unknown6_vecs + unknown7_vecs\n",
    "all_unknown = np.array(all_unknown)\n",
    "print(f\"Total Unknown Mirror Subflows: {len(all_unknown)}\")\n",
    "\n",
    "# Saving unknown feature vectors, all in one array\n",
    "np.save('../../Feature-Vectors/Basic-Features/mirror_unknown_100_packet_subflow_features.npy', all_unknown)\n",
    "\n",
    "# Saving a list of dicts of unknown feature vectors, dicts map flows to feature vectors\n",
    "# Subflow feature vectors are chronologically ordered in their lists, and each list mapped to its flow\n",
    "np.save('../../Feature-Vectors/Basic-Features/mirror_unknown_100_packet_subflow_flow_features.npy', unknown_flow_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Chronological Ordering / Order of Packets in CSV is Perserved in Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n",
      "[(27.257798377, 27.973901661), (27.973926918, 28.120795314), (28.120842313, 28.12531537), (28.125343187, 28.322537352), (28.322538533, 28.331980117), (28.331996418, 28.37444237), (28.374461247, 28.375774029), (28.375775632, 28.383764782), (28.383779927, 28.387428529), (28.387482157, 28.390666635)]\n",
      "\n",
      "\n",
      "[(27.323444366, 28.01450188), (28.014513628, 28.122722325), (28.122805275, 28.326296094), (28.326310126, 28.380773065), (28.380788228, 28.383948747), (28.384001942, 28.799189635), (28.799217777, 28.805848382), (28.805876488, 28.813834709), (28.813870127, 28.855042837), (28.855172411, 28.862257084)]\n",
      "\n",
      "\n",
      "[(27.49484936, 28.025134936), (28.025532065, 28.125593841), (28.12565499, 28.33215705), (28.332216172, 28.383373023), (28.383434527, 28.42495364), (28.424985874, 28.805421637), (28.805481564, 28.811565872), (28.811622517, 28.819563815), (28.819628469, 28.858871189), (28.858933799, 28.865318809)]\n",
      "\n",
      "\n",
      "[(27.323443787, 28.10619484), (28.106212868, 28.157527011), (28.157556197, 28.376562637), (28.376589193, 28.382730446), (28.382768225, 28.386479901), (28.38651421, 28.800698223), (28.800742176, 28.810797225), (28.810900996, 28.859056291), (28.85907527, 28.866082257), (28.866207217, 28.878608076)]\n",
      "\n",
      "\n",
      "[(27.566169386, 28.1070361), (28.107098582, 28.330000978), (28.330084001, 28.385812636), (28.385875142, 28.805874896), (28.805934492, 28.814093842), (28.814160327, 28.866114182), (28.866165943, 28.874492666), (28.874855633, 28.880932261), (28.880993707, 28.889388556), (28.889395655, 29.653686591)]\n",
      "\n",
      "\n",
      "[(27.566139878, 28.121404495), (28.121465292, 28.326814189), (28.326905566, 28.382194542), (28.382253764, 28.802443088), (28.802503678, 28.813892187), (28.813961659, 28.856528404), (28.856586716, 28.865654217), (28.865715355, 28.875427611), (28.875542416, 28.884860832), (28.8849832, 29.65493996)]\n",
      "\n",
      "\n",
      "[(27.571865495, 28.295262342), (28.295263027, 28.866771334), (28.967837252, 34.746243555), (34.746312202, 34.750630799), (34.750653237, 34.753963457), (34.753995924, 34.795006388), (34.795018955, 34.801660837), (34.801714344, 34.804729714), (34.804731694, 34.807227295), (34.807247987, 35.09394068)]\n",
      "\n",
      "\n",
      "[(27.514334463, 28.323952366), (28.323953174, 28.377390514), (28.377392358, 28.378960559), (28.378961222, 28.429774318), (28.429775391, 28.431124166), (28.431136274, 28.796924267), (28.796945055, 28.808445877), (28.808571597, 28.852822991), (28.852877371, 28.85770742), (28.857810804, 28.863876128)]\n",
      "\n",
      "\n",
      "[(27.257721965, 28.331066012), (28.331126562, 28.853667933), (28.853697403, 28.876844012), (28.876898175, 28.932976461), (28.933019766, 30.838873336), (30.841060611, 30.942356349), (30.942435501, 31.000576892), (31.000605099, 31.15472758), (31.157040994, 31.212307886), (31.212345266, 31.320460607)]\n",
      "\n",
      "\n",
      "[(27.814074299, 28.34777964), (28.347861769, 28.71021547), (28.81590655, 34.745751019), (34.745812587, 34.753408473), (34.753467738, 34.800243062), (34.800366786, 35.094576435), (35.094682953, 35.103887379), (35.103950095, 35.149686933), (35.149735886, 35.156673578), (35.156733979, 35.168700927)]\n",
      "\n",
      "\n",
      "[(27.567291735, 28.377330062), (28.377364679, 28.796727622), (28.796802613, 28.807372333), (28.80742999, 28.851464391), (28.851495974, 28.85794631), (28.857997634, 28.867513068), (28.867634726, 28.874809654), (28.874931137, 28.883573078), (28.88363168, 28.892952881), (28.893073697, 29.864408335)]\n",
      "\n",
      "\n",
      "[(27.701791654, 28.393132756), (28.39318162, 33.173032074), (33.276799336, 34.74953222), (34.749659334, 34.798134395), (34.798190633, 35.092186274), (35.092213656, 35.103509301), (35.103573394, 35.151366373), (35.151430052, 35.159665861), (35.159730316, 35.167643133), (35.167703888, 35.176355255)]\n",
      "\n",
      "\n",
      "[(27.323354515, 28.803478945), (28.803526956, 28.859587317), (28.859658262, 28.886407284), (28.886445585, 29.66241349), (29.662499265, 29.722609114), (29.72267315, 29.783380374), (29.783382468, 29.810270555), (29.810332461, 29.838675437), (29.838736431, 30.816293592), (30.817079469, 30.862718816)]\n",
      "\n",
      "\n",
      "[(27.323443822, 28.810627558), (28.810713998, 28.868720842), (28.868778299, 29.662535398), (29.662595807, 29.710639165), (29.711256047, 29.76549454), (29.765556294, 29.79783281), (29.797882865, 29.828658024), (29.828718364, 30.790141731), (30.790628934, 30.863714221), (30.863751493, 31.405861503)]\n",
      "\n",
      "\n",
      "[(27.514262876, 28.861954836), (28.863991875, 28.885217975), (28.88528068, 30.945158487), (30.945220423, 31.25476555), (31.260261804, 31.467993315), (31.468054705, 32.209553274), (32.209611181, 32.361808892), (32.361898074, 32.566772718), (32.570448668, 33.355468231), (33.355510528, 33.466246634)]\n",
      "\n",
      "\n",
      "[(27.504084298, 29.488795384), (29.591772498, 34.745090482), (34.745120694, 34.749908161), (34.749952332, 34.787800809), (34.7948571, 34.799462199), (34.799503134, 34.803796077), (34.8038165, 34.806305935), (34.806334506, 35.092289854), (35.092302554, 35.0977328), (35.097819335, 35.104177417)]\n",
      "\n",
      "\n",
      "[(27.814101742, 33.579028693), (33.579088965, 34.79591362), (34.795957893, 34.845798709), (35.094578354, 35.14878749), (35.148823894, 35.157820066), (35.15789564, 35.165237469), (35.165302903, 35.200816028), (35.200860041, 35.792281717), (35.792319288, 35.803385585), (35.80349009, 35.818213753)]\n",
      "\n",
      "\n",
      "[(27.814108782, 34.383424063), (34.488865052, 34.801354609), (34.801421313, 35.103883966), (35.103905484, 35.154779556), (35.154849973, 35.160958749), (35.161012914, 35.167237349), (35.167422952, 35.173679798), (35.173697368, 35.801210063), (35.801325696, 35.813914526), (35.81396853, 35.851942132)]\n",
      "\n",
      "\n",
      "[(27.57190559, 34.743505116), (34.743541388, 34.796344164), (34.796390062, 34.800054188), (34.800084235, 35.09539888), (35.095414412, 35.099771291), (35.099872299, 35.155155672), (35.15526149, 35.166844903), (35.166951513, 35.202499349), (35.202511534, 35.204271925), (35.204292219, 35.205863708)]\n",
      "\n",
      "\n",
      "[(27.571907181, 34.744893415), (34.744992262, 34.799048204), (34.799048862, 35.098469837), (35.098526208, 35.10316706), (35.103209306, 35.106996236), (35.107020838, 35.157532566), (35.15760504, 35.169179224), (35.169272928, 35.797793089), (35.797845778, 35.80337596), (35.803381226, 35.813913398)]\n",
      "\n",
      "\n",
      "[(27.503989196, 34.803215393), (34.803220443, 35.154832897), (35.154891772, 35.819962538), (35.820022677, 35.861048579), (35.861110942, 36.078572056), (36.127331505, 36.286313012), (36.286327339, 36.930223128), (36.930270356, 36.977752155), (36.977937352, 37.029421281), (37.029424866, 37.039522057)]\n",
      "\n",
      "\n",
      "[(27.571776, 35.100916359), (35.103634704, 35.170542849), (35.170603053, 35.826342435), (35.826407233, 35.874920391), (35.874933107, 35.900849112), (35.900869818, 35.910902935), (35.910965134, 35.927000028), (35.927123609, 37.06505195), (37.065136573, 38.057033648), (38.057094376, 38.107947947)]\n",
      "\n",
      "\n",
      "[(27.571863889, 35.102962024), (35.103041772, 35.817331512), (35.817946532, 35.861267485), (35.861322108, 35.955992963), (35.956005077, 36.983790182), (36.983907866, 37.090895937), (37.094647596, 38.053419701), (38.053481554, 38.060452174), (38.060500078, 38.108708582), (38.108816507, 38.114119293)]\n",
      "\n",
      "\n",
      "[(27.571817425, 35.151767657), (35.151828127, 35.172182026), (35.172223356, 35.815254669), (35.815303703, 35.873252043), (35.873313939, 35.953536379), (35.953811507, 36.981920583), (36.982007797, 37.096838169), (37.096966596, 38.053183573), (38.053244888, 38.064809627), (38.064866466, 38.109822895)]\n",
      "\n",
      "\n",
      "[(548.994674026, 549.460683165), (549.460703922, 549.566775119), (549.566836587, 549.849827533), (549.849883606, 555.539835748), (555.539867102, 555.592607468), (555.592685954, 555.645329166), (555.645366085, 555.937386685), (555.937496228, 555.99009905), (555.990225797, 556.007114098), (556.007242294, 556.021565496)]\n",
      "\n",
      "\n",
      "[(548.834648527, 549.479134334), (549.47932413, 555.540055687), (555.540057613, 555.544848363), (555.544851334, 555.547389122), (555.547391132, 555.591389199), (555.591442197, 555.592757697), (555.592758805, 555.598489126), (555.598509582, 555.599909379), (555.599911081, 555.601132007), (555.601225021, 555.646307519)]\n",
      "\n",
      "\n",
      "[(548.99463053, 549.531469672), (549.531504483, 549.759446481), (549.759512142, 555.5430026), (555.543033019, 555.596044664), (555.596135853, 555.936572716), (555.936630977, 555.94439389), (555.944455468, 555.990890027), (555.990940369, 556.001078076), (556.001134456, 556.009585019), (556.009634097, 556.017501035)]\n",
      "\n",
      "\n",
      "[(548.82083248, 549.565916637), (549.60306586, 555.543404212), (555.543405598, 555.544652323), (555.544690938, 555.547314493), (555.547340709, 555.595731812), (555.595733499, 555.59698781), (555.59700042, 555.598467575), (555.59849525, 555.932489453), (555.932505022, 555.935653763), (555.935704266, 555.939647981)]\n",
      "\n",
      "\n",
      "[(549.20887981, 549.850547267), (549.850548951, 549.906125288), (549.906154429, 549.922513108), (549.922563726, 549.957527613), (549.957547027, 549.972790441), (549.972802538, 550.108386932), (550.108420227, 550.114198495), (550.114271655, 550.154990545), (550.154992267, 550.156256319), (550.156271554, 550.165018568)]\n",
      "\n",
      "\n",
      "[(549.203953217, 549.862791691), (549.862812201, 549.918052001), (549.918083744, 549.921871325), (549.92194453, 549.969527569), (549.969530479, 549.971677133), (549.971743767, 550.107478394), (550.107511552, 550.112543272), (550.112598249, 550.158051361), (550.158052989, 550.159537842), (550.159574955, 550.162217631)]\n",
      "\n",
      "\n",
      "[(549.415647726, 549.923252972), (549.92336062, 550.10457171), (550.10464062, 550.114147869), (550.114267478, 550.410423788), (550.410435287, 550.420967991), (550.421101845, 550.43374272), (550.461587195, 550.477170817), (550.4772995, 550.9644754), (550.96459601, 550.979302954), (550.979813815, 550.994552222)]\n",
      "\n",
      "\n",
      "[(549.415617629, 549.925168039), (549.925293769, 550.109378732), (550.109491888, 550.164517029), (550.164639423, 550.414050015), (550.414171557, 550.428597467), (550.428710241, 550.47097788), (550.471110338, 550.483237215), (550.483364517, 550.974654583), (550.9747824, 550.986650906), (550.986779142, 551.001698733)]\n",
      "\n",
      "\n",
      "[(549.20896619, 549.972064768), (549.972067948, 550.159028494), (550.159141227, 550.418739224), (550.418816761, 550.472377153), (550.472388219, 550.477409977), (550.477417941, 550.51963412), (550.519694419, 550.523655912), (550.523692005, 550.526336995), (550.526338983, 550.963843417), (550.96384507, 550.97652323)]\n",
      "\n",
      "\n",
      "[(549.209049936, 550.106851556), (550.106852981, 550.161067146), (550.161069514, 550.418650005), (550.418753248, 550.472213958), (550.472259401, 550.477229373), (550.477285516, 550.521112889), (550.521144826, 550.523625051), (550.523646662, 550.526438738), (550.526459536, 550.976103267), (550.976248323, 550.987225912)]\n",
      "\n",
      "\n",
      "[(549.415643426, 550.10998766), (550.110046432, 550.419688646), (550.41974648, 550.473329224), (550.47338619, 550.478974993), (550.47903498, 550.523804681), (550.523928925, 550.974693717), (550.974822361, 550.986280599), (550.986340033, 550.993787804), (550.993835129, 550.999504554), (550.999523337, 551.026871151)]\n",
      "\n",
      "\n",
      "[(549.41564497, 550.413472087), (550.413577628, 550.475494679), (550.475630896, 550.523612128), (550.523672259, 550.972603473), (550.972725396, 550.993538772), (550.99360106, 551.022865637), (551.02300549, 551.033053129), (551.033184323, 551.044541626), (551.044668841, 551.067454389), (551.0675661, 551.079245026)]\n",
      "\n",
      "\n",
      "[(549.203907611, 550.473306073), (550.473434235, 551.048928372), (551.049071621, 551.071120404), (551.071123327, 552.699164412), (552.699325637, 552.748923013), (552.748929646, 552.801936651), (552.801990995, 552.908949604), (552.909716035, 552.968307534), (552.968368884, 553.116449784), (553.116485464, 553.160638487)]\n",
      "\n",
      "\n",
      "[(549.209012208, 550.482703383), (550.48275856, 551.003194301), (551.003245312, 551.035838015), (551.035899547, 551.105878262), (551.105971079, 551.860323989), (551.86038405, 551.875260765), (551.87526191, 551.894864206), (551.894924469, 551.922316742), (551.922716732, 551.964744848), (551.964747301, 551.996675449)]\n",
      "\n",
      "\n",
      "[(549.208839613, 550.971559547), (550.971693674, 551.039769366), (551.039776755, 551.050600824), (551.050603843, 551.076929618), (551.077062169, 551.911765996), (552.072977069, 552.701744617), (552.701872322, 552.799395907), (552.799455131, 552.866488039), (552.866681783, 553.013025464), (553.01306961, 553.130063545)]\n",
      "\n",
      "\n",
      "[(549.20893269, 550.981235627), (550.981358476, 551.084875348), (551.084894919, 551.862253715), (551.862304269, 551.874398656), (551.874460312, 551.892242645), (551.892302761, 551.916571182), (551.916658264, 551.927566523), (551.927568548, 551.955094661), (551.955154719, 551.975355209), (551.975417123, 551.997978094)]\n",
      "\n",
      "\n",
      "[(548.994605564, 551.956534424), (551.956589665, 555.936089262), (555.936171203, 555.994599572), (555.994730257, 556.008662117), (556.008793614, 556.672313158), (556.672418546, 556.690800523), (556.690925352, 556.733013499), (556.733105327, 556.74181672), (556.741943883, 556.750669881), (556.750831822, 556.759953747)]\n",
      "\n",
      "\n",
      "[(548.994671827, 553.327148617), (553.327157913, 555.984552314), (555.984680473, 555.997309523), (555.997438868, 556.041974787), (556.042062257, 556.049965644), (556.050026216, 556.667645578), (556.667705653, 556.683024434), (556.683183182, 556.695886416), (556.695943123, 556.703909814), (556.703960937, 556.722284102)]\n",
      "\n",
      "\n",
      "[(548.820694719, 554.969419249), (555.076512063, 555.996793789), (555.99685503, 556.016636029), (556.016697359, 556.828614554), (556.828645019, 558.160558596), (558.16074104, 558.231089029), (558.231212351, 558.325199013), (558.340254, 559.343576657), (559.343664528, 559.446462892), (559.446504522, 559.459000584)]\n",
      "\n",
      "\n",
      "[(548.834722757, 555.541406332), (555.541408718, 555.594443046), (555.594455324, 555.930152922), (555.93026183, 555.935669869), (555.935791153, 555.988690494), (555.988815095, 556.000477711), (556.000651796, 556.042768964), (556.042801203, 556.045954303), (556.04596713, 556.048418605), (556.048419427, 556.050872246)]\n",
      "\n",
      "\n",
      "[(548.834780392, 555.542149006), (555.542162091, 555.934975982), (555.93505164, 555.939093796), (555.939125916, 555.942876267), (555.942937468, 555.995214003), (555.995331876, 556.004111613), (556.004240151, 556.665925223), (556.665946648, 556.67452535), (556.674622578, 556.685130437), (556.685240294, 556.724045803)]\n",
      "\n",
      "\n",
      "[(548.834523764, 555.935775742), (555.935850118, 556.668560379), (556.668623806, 556.704561015), (556.704611288, 556.722431901), (556.730195106, 556.756679683), (556.756737497, 556.766150517), (556.766207573, 556.773842129), (556.773961443, 556.788421453), (556.788544611, 558.113587886), (558.113705964, 558.214513398)]\n",
      "\n",
      "\n",
      "[(548.83457712, 556.669093648), (556.669213852, 556.748407938), (556.748509405, 556.757662596), (556.757774336, 556.836059706), (556.836120609, 558.115259298), (558.124937379, 558.232430809), (558.232491764, 558.28043859), (558.280550231, 558.306644765), (558.31029891, 558.360413128), (558.360479199, 559.357368948)]\n",
      "\n",
      "\n",
      "[(548.834653511, 556.69323163), (556.69335658, 556.748426859), (556.75248924, 556.802853125), (556.80298048, 556.841578188), (556.841638816, 556.863810299), (556.863869705, 558.094511461), (558.094581978, 558.128033757), (558.128163403, 558.202332418), (558.202394934, 558.251954165), (558.252017602, 558.27934513)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chronological ordering is verfied :)\n",
    "# The start times for all subflows before some subflow are all before the subflow - though tiny margins\n",
    "data_dir = \"../../DT-Data/new-globus/\"\n",
    "N = 100\n",
    "\n",
    "globus_10_min_vecs, globus_10_min_flow_vecs, times = extract_packet_win_features(data_dir + 'globus-10-min.csv', N)\n",
    "print(len(globus_10_min_flow_vecs), len(times))\n",
    "for flow in globus_10_min_flow_vecs:\n",
    "#     print(len(globus_10_min_flow_vecs[flow]), len(times[flow])) # These are the same :)\n",
    "#     print(globus_10_min_flow_vecs[flow][:10])\n",
    "    print(times[flow][:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is older data (first experiments, with ~3600 unknown and more known - known had FDT, RClone, and Globus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 Packet Subflow Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Globus:\n",
      "3571\n",
      "5289\n",
      "2141\n",
      "4990\n",
      "2649\n",
      "6061\n",
      "\n",
      "FDT:\n",
      "3126\n",
      "5792\n",
      "6078\n",
      "6025\n",
      "6578\n",
      "3580\n",
      "\n",
      "RClone:\n",
      "3615\n",
      "597\n",
      "4159\n",
      "616\n",
      "5833\n",
      "898\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../../DT-Data/\"\n",
    "N = 100\n",
    "\n",
    "# Globus\n",
    "globus_dtn1_src1 = extract_packet_win_features(data_dir + 'globus-dtn1-src-iso.csv', N)\n",
    "print(\"Globus:\")\n",
    "print(len(globus_dtn1_src1))\n",
    "\n",
    "globus_dtn1_dest1 = extract_packet_win_features(data_dir + 'globus-dtn1-dest-iso.csv', N)\n",
    "print(len(globus_dtn1_dest1))\n",
    "\n",
    "globus_dtn1_src2 = extract_packet_win_features(data_dir + 'globus-dtn1-src-iso2.csv', N)\n",
    "print(len(globus_dtn1_src2))\n",
    "\n",
    "globus_dtn1_dest2 = extract_packet_win_features(data_dir + 'globus-dtn1-dest-iso2.csv', N)\n",
    "print(len(globus_dtn1_dest2))\n",
    "\n",
    "globus_clusterdtn_src = extract_packet_win_features(data_dir + 'globus-clusterdtn-src-iso.csv', N)\n",
    "print(len(globus_clusterdtn_src))\n",
    "\n",
    "globus_clusterdtn_dest = extract_packet_win_features(data_dir + 'globus-clusterdtn-dest-iso.csv', N)\n",
    "print(len(globus_clusterdtn_dest))\n",
    "\n",
    "\n",
    "# FDT\n",
    "fdt_a2_src = extract_packet_win_features(data_dir + 'fdt-airplane2-src-iso.csv', N)\n",
    "print(\"\\nFDT:\")\n",
    "print(len(fdt_a2_src))\n",
    "\n",
    "fdt_a2_dest = extract_packet_win_features(data_dir + 'fdt-airplane2-dest-iso.csv', N)\n",
    "print(len(fdt_a2_dest))\n",
    "\n",
    "fdt_a2_dest_1str = extract_packet_win_features(data_dir + 'fdt-airplane2-dest-iso-1stream.csv', N)\n",
    "print(len(fdt_a2_dest_1str))\n",
    "\n",
    "fdt_a2_dest_2str = extract_packet_win_features(data_dir + 'fdt-airplane2-dest-iso-2stream.csv', N)\n",
    "print(len(fdt_a2_dest_2str))\n",
    "\n",
    "fdt_dtn1_dest = extract_packet_win_features(data_dir + 'fdt-dtn1-dest-iso.csv', N)\n",
    "print(len(fdt_dtn1_dest))\n",
    "\n",
    "fdt_dtn1_src = extract_packet_win_features(data_dir + 'fdt-dtn1-src-iso.csv', N)\n",
    "print(len(fdt_dtn1_src))\n",
    "\n",
    "\n",
    "# RClone\n",
    "rclone_src = extract_packet_win_features(data_dir + 'rclone-gdrive-src-iso.csv', N)\n",
    "print(\"\\nRClone:\")\n",
    "print(len(rclone_src))\n",
    "\n",
    "rclone_dest = extract_packet_win_features(data_dir + 'rclone-gdrive-dest-iso.csv', N)\n",
    "print(len(rclone_dest))\n",
    "\n",
    "rclone_src2 = extract_packet_win_features(data_dir + 'rclone-gdrive-src-iso2.csv', N)\n",
    "print(len(rclone_src2))\n",
    "\n",
    "rclone_dest2 = extract_packet_win_features(data_dir + 'rclone-gdrive-dest-iso2.csv', N)\n",
    "print(len(rclone_dest2))\n",
    "\n",
    "rclone_src3 = extract_packet_win_features(data_dir + 'rclone-gdrive-src-iso3.csv', N)\n",
    "print(len(rclone_src3))\n",
    "\n",
    "rclone_dest3 = extract_packet_win_features(data_dir + 'rclone-gdrive-dest-iso3.csv', N)\n",
    "print(len(rclone_dest3))\n",
    "\n",
    "#Creating dictionary of all 100 packet subflow feature vectors mapping by capture file, saving it\n",
    "hundred_N_dict = {'globus_dtn1_src1' : globus_dtn1_src1, 'globus_dtn1_dest1' : globus_dtn1_dest1, \n",
    "             'globus_dtn1_src2' : globus_dtn1_src2, 'globus_dtn1_dest2' : globus_dtn1_dest2,\n",
    "             'globus_clusterdtn_src' : globus_clusterdtn_src, 'globus_clusterdtn_dest' : globus_clusterdtn_dest,\n",
    "             'fdt_a2_src' : fdt_a2_src, 'fdt_a2_dest' : fdt_a2_src, 'fdt_dtn1_dest' : fdt_dtn1_dest, \n",
    "             'fdt_dtn1_src' : fdt_dtn1_src, 'fdt_a2_dest_1str' : fdt_a2_dest_1str, 'fdt_a2_dest_2str' : fdt_a2_dest_2str,\n",
    "             'rclone_src' : rclone_src, 'rclone_dest' : rclone_dest, 'rclone_src2': rclone_src2, \n",
    "             'rclone_dest2': rclone_dest2, 'rclone_src3' : rclone_src3, 'rclone_dest3' : rclone_dest3}\n",
    "\n",
    "np.save('../../Feature-Vectors/100_packet_subflow_features.npy', hundred_N_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from mirror_unknown1.csv: 828\n",
      "Number of flow feature vectors from mirror_unknown2.csv: 914\n",
      "Number of flow feature vectors from dtn1_unknown1.csv: 363\n",
      "Number of flow feature vectors from dtn1_unknown2.csv: 387\n",
      "Number of flow feature vectors from dtn1_unknown3.csv: 655\n",
      "Number of flow feature vectors from mirror_unknown3.csv: 556\n"
     ]
    }
   ],
   "source": [
    "# UNKNOWN/MIRROR CAPTURED DATA 100 packet subflow FEATURIZATION\n",
    "uknown_data_dir = \"../../Unknown-Data/\"\n",
    "N = 100\n",
    "\n",
    "# Only extracting 4 second windows\n",
    "unknown1 = uknown_data_dir + 'mirror_unknown1.csv'\n",
    "unknown1 = extract_packet_win_features(unknown1, N)\n",
    "print(f'Number of flow feature vectors from mirror_unknown1.csv: {len(unknown1)}')\n",
    "\n",
    "unknown2 = uknown_data_dir + 'mirror_unknown2.csv'\n",
    "unknown2 = extract_packet_win_features(unknown2, N)\n",
    "print(f'Number of flow feature vectors from mirror_unknown2.csv: {len(unknown2)}')\n",
    "\n",
    "unknown3 = uknown_data_dir + 'dtn1_unknown1.csv'\n",
    "unknown3 = extract_packet_win_features(unknown3, N)\n",
    "print(f'Number of flow feature vectors from dtn1_unknown1.csv: {len(unknown3)}')\n",
    "\n",
    "unknown4 = uknown_data_dir + 'dtn1_unknown2.csv'\n",
    "unknown4 = extract_packet_win_features(unknown4, N)\n",
    "print(f'Number of flow feature vectors from dtn1_unknown2.csv: {len(unknown4)}')\n",
    "\n",
    "unknown5 = uknown_data_dir + 'dtn1_unknown3.csv'\n",
    "unknown5 = extract_packet_win_features(unknown5, N)\n",
    "print(f'Number of flow feature vectors from dtn1_unknown3.csv: {len(unknown5)}')\n",
    "\n",
    "unknown6 = uknown_data_dir + 'mirror_unknown3.csv'\n",
    "unknown6 = extract_packet_win_features(unknown6, N)\n",
    "print(f'Number of flow feature vectors from mirror_unknown3.csv: {len(unknown6)}')\n",
    "\n",
    "# Dictionary of all unknown feature vectors\n",
    "hundred_unknown_dict = {'unknown1': unknown1, 'unknown2': unknown2, 'unknown3': unknown3, \n",
    "                'unknown4': unknown4, 'unknown5': unknown5, 'unknown6': unknown6}\n",
    "\n",
    "np.save('../../Feature-Vectors/unknown_100_packet_subflow_features.npy', hundred_unknown_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25 Packet Subflow Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Globus:\n",
      "14286\n",
      "21177\n",
      "8567\n",
      "19976\n",
      "10603\n",
      "24259\n",
      "\n",
      "FDT:\n",
      "12517\n",
      "23177\n",
      "24315\n",
      "24104\n",
      "26321\n",
      "14332\n",
      "\n",
      "RClone:\n",
      "14464\n",
      "2392\n",
      "16644\n",
      "2468\n",
      "23335\n",
      "3599\n"
     ]
    }
   ],
   "source": [
    "N = 25\n",
    "\n",
    "# Globus\n",
    "globus_dtn1_src1 = extract_packet_win_features(data_dir + 'globus-dtn1-src-iso.csv', N)\n",
    "print(\"Globus:\")\n",
    "print(len(globus_dtn1_src1))\n",
    "\n",
    "globus_dtn1_dest1 = extract_packet_win_features(data_dir + 'globus-dtn1-dest-iso.csv', N)\n",
    "print(len(globus_dtn1_dest1))\n",
    "\n",
    "globus_dtn1_src2 = extract_packet_win_features(data_dir + 'globus-dtn1-src-iso2.csv', N)\n",
    "print(len(globus_dtn1_src2))\n",
    "\n",
    "globus_dtn1_dest2 = extract_packet_win_features(data_dir + 'globus-dtn1-dest-iso2.csv', N)\n",
    "print(len(globus_dtn1_dest2))\n",
    "\n",
    "globus_clusterdtn_src = extract_packet_win_features(data_dir + 'globus-clusterdtn-src-iso.csv', N)\n",
    "print(len(globus_clusterdtn_src))\n",
    "\n",
    "globus_clusterdtn_dest = extract_packet_win_features(data_dir + 'globus-clusterdtn-dest-iso.csv', N)\n",
    "print(len(globus_clusterdtn_dest))\n",
    "\n",
    "\n",
    "# FDT\n",
    "fdt_a2_src = extract_packet_win_features(data_dir + 'fdt-airplane2-src-iso.csv', N)\n",
    "print(\"\\nFDT:\")\n",
    "print(len(fdt_a2_src))\n",
    "\n",
    "fdt_a2_dest = extract_packet_win_features(data_dir + 'fdt-airplane2-dest-iso.csv', N)\n",
    "print(len(fdt_a2_dest))\n",
    "\n",
    "fdt_a2_dest_1str = extract_packet_win_features(data_dir + 'fdt-airplane2-dest-iso-1stream.csv', N)\n",
    "print(len(fdt_a2_dest_1str))\n",
    "\n",
    "fdt_a2_dest_2str = extract_packet_win_features(data_dir + 'fdt-airplane2-dest-iso-2stream.csv', N)\n",
    "print(len(fdt_a2_dest_2str))\n",
    "\n",
    "fdt_dtn1_dest = extract_packet_win_features(data_dir + 'fdt-dtn1-dest-iso.csv', N)\n",
    "print(len(fdt_dtn1_dest))\n",
    "\n",
    "fdt_dtn1_src = extract_packet_win_features(data_dir + 'fdt-dtn1-src-iso.csv', N)\n",
    "print(len(fdt_dtn1_src))\n",
    "\n",
    "\n",
    "# RClone\n",
    "rclone_src = extract_packet_win_features(data_dir + 'rclone-gdrive-src-iso.csv', N)\n",
    "print(\"\\nRClone:\")\n",
    "print(len(rclone_src))\n",
    "\n",
    "rclone_dest = extract_packet_win_features(data_dir + 'rclone-gdrive-dest-iso.csv', N)\n",
    "print(len(rclone_dest))\n",
    "\n",
    "rclone_src2 = extract_packet_win_features(data_dir + 'rclone-gdrive-src-iso2.csv', N)\n",
    "print(len(rclone_src2))\n",
    "\n",
    "rclone_dest2 = extract_packet_win_features(data_dir + 'rclone-gdrive-dest-iso2.csv', N)\n",
    "print(len(rclone_dest2))\n",
    "\n",
    "rclone_src3 = extract_packet_win_features(data_dir + 'rclone-gdrive-src-iso3.csv', N)\n",
    "print(len(rclone_src3))\n",
    "\n",
    "rclone_dest3 = extract_packet_win_features(data_dir + 'rclone-gdrive-dest-iso3.csv', N)\n",
    "print(len(rclone_dest3))\n",
    "\n",
    "# Creating dictionary of all 25 packet subflow feature vectors mapping by capture file, saving it\n",
    "twentyfive_N_dict = {'globus_dtn1_src1' : globus_dtn1_src1, 'globus_dtn1_dest1' : globus_dtn1_dest1, \n",
    "             'globus_dtn1_src2' : globus_dtn1_src2, 'globus_dtn1_dest2' : globus_dtn1_dest2,\n",
    "             'globus_clusterdtn_src' : globus_clusterdtn_src, 'globus_clusterdtn_dest' : globus_clusterdtn_dest,\n",
    "             'fdt_a2_src' : fdt_a2_src, 'fdt_a2_dest' : fdt_a2_src, 'fdt_dtn1_dest' : fdt_dtn1_dest, \n",
    "             'fdt_dtn1_src' : fdt_dtn1_src, 'fdt_a2_dest_1str' : fdt_a2_dest_1str, 'fdt_a2_dest_2str' : fdt_a2_dest_2str,\n",
    "             'rclone_src' : rclone_src, 'rclone_dest' : rclone_dest, 'rclone_src2': rclone_src2, \n",
    "             'rclone_dest2': rclone_dest2, 'rclone_src3' : rclone_src3, 'rclone_dest3' : rclone_dest3}\n",
    "\n",
    "np.save('../../Feature-Vectors/25_packet_subflow_features.npy', twentyfive_N_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flow feature vectors from mirror_unknown1.csv: 3315\n",
      "Number of flow feature vectors from mirror_unknown2.csv: 3656\n",
      "Number of flow feature vectors from dtn1_unknown1.csv: 1462\n",
      "Number of flow feature vectors from dtn1_unknown2.csv: 1555\n",
      "Number of flow feature vectors from dtn1_unknown3.csv: 2637\n",
      "Number of flow feature vectors from mirror_unknown3.csv: 2239\n"
     ]
    }
   ],
   "source": [
    "# UNKNOWN/MIRROR CAPTURED DATA 25 packet subflow FEATURIZATION\n",
    "uknown_data_dir = \"../../Unknown-Data/\"\n",
    "N = 25\n",
    "\n",
    "# Only extracting 4 second windows\n",
    "unknown1 = uknown_data_dir + 'mirror_unknown1.csv'\n",
    "unknown1 = extract_packet_win_features(unknown1, N)\n",
    "print(f'Number of flow feature vectors from mirror_unknown1.csv: {len(unknown1)}')\n",
    "\n",
    "unknown2 = uknown_data_dir + 'mirror_unknown2.csv'\n",
    "unknown2 = extract_packet_win_features(unknown2, N)\n",
    "print(f'Number of flow feature vectors from mirror_unknown2.csv: {len(unknown2)}')\n",
    "\n",
    "unknown3 = uknown_data_dir + 'dtn1_unknown1.csv'\n",
    "unknown3 = extract_packet_win_features(unknown3, N)\n",
    "print(f'Number of flow feature vectors from dtn1_unknown1.csv: {len(unknown3)}')\n",
    "\n",
    "unknown4 = uknown_data_dir + 'dtn1_unknown2.csv'\n",
    "unknown4 = extract_packet_win_features(unknown4, N)\n",
    "print(f'Number of flow feature vectors from dtn1_unknown2.csv: {len(unknown4)}')\n",
    "\n",
    "unknown5 = uknown_data_dir + 'dtn1_unknown3.csv'\n",
    "unknown5 = extract_packet_win_features(unknown5, N)\n",
    "print(f'Number of flow feature vectors from dtn1_unknown3.csv: {len(unknown5)}')\n",
    "\n",
    "unknown6 = uknown_data_dir + 'mirror_unknown3.csv'\n",
    "unknown6 = extract_packet_win_features(unknown6, N)\n",
    "print(f'Number of flow feature vectors from mirror_unknown3.csv: {len(unknown6)}')\n",
    "\n",
    "# Dictionary of all unknown feature vectors\n",
    "twenty_five_unknown_dict = {'unknown1': unknown1, 'unknown2': unknown2, 'unknown3': unknown3, \n",
    "                'unknown4': unknown4, 'unknown5': unknown5, 'unknown6': unknown6}\n",
    "\n",
    "np.save('../../Feature-Vectors/unknown_25_packet_subflow_features.npy', twenty_five_unknown_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
